<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
<title>CSE 344, section 9: Hadoop and Pig for Homework 6</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<style type="text/css">
  body { font-family: Helvetica, Arial, "Liberation Sans", sans-serif }
  p, li { margin: 0px }
  ul { list-style-type: disc }
  ul ul { list-style-type: circle }
</style>
</head>

<body>

<h1>Plan for Section 9: Using AWS Hadoop, Pig</h1>

<p>Before section:
<ul>
  <li>Change source and dest. paths in our <code>example.pig</code> script
    to refer to our S3 input bucket and my S3 output bucket (keep backups)
  <li>Uninstall FoxyProxy and delete <code>foxyproxy.xml</code> file
  <li>That morning: 
  <ul>
    <li>Start up an AWS cluster for Pig (2 nodes?).
    <li>Run <code>example.pig</code> on the cluster.
    <li>Verify that the <code>example.pig</code> is accessible via
      S3 Management Console.
  </ul>
</ul>

<p>2-3 mins: Introduction
<ul>
  <li>This is mechanics - using Pig and AWS Hadoop clusters
  <li>Not much Pig Latin - see lecture for more
  <li>Everything here is in the 
    <a href="http://www.cs.washington.edu/education/courses/cse344/11sp/hw/hw6/hw6.html">
      project instructions</a>
  (<a href="http://www.cs.washington.edu/education/courses/cse344/11sp/hw/hw6/hw6-awsusage.html">AWS usage guide</a>)
  <ul>
    <li>No slides
    <li>Just demos - hopefully you've at least tried most of this already
  </ul>
  </li>
</ul>

<p>10 mins: AWS and cluster setup through the web
<a href="https://console.aws.amazon.com/elasticmapreduce/home">Management Console</a>
<ul>
  <li>Create an EC2 ssh key-pair (but use the old one)
  <li>Create an Elastic MapReduce cluster
  <ul>
    <li>But don't actually do it - use the one created earlier
    <li>It takes ~5 mins to start up (say 5-10)
  </ul>
  <li>Connecting to the cluster master node
  <li>Shut down the cluster (again, don't actually do it)
  <ul>
    <li>Again, it takes about 5-10 minutes
  </ul>
  </li>
</ul>

<p>10 mins: Input and output concepts.<br>
<em>Input:</em>
<ul>
  <li>
    Your input files come from Amazon S3, which is different from
    the Hadoop DFS natively supported by Hadoop.  The AWS Hadoop cluster 
    maintains its own HDFS instance, which dies with the cluster 
    (this fact is not inherent in HDFS, though); but S3 persists
    forever.  Hadoop supports both; but S3 obviously works fastest
    when your cluster is on EC2 or Elastic Mapreduce.
   <li>
    (Aside: s3n: protocol for hadoop
    means S3 without Hadoop-managed file chunking, maximum file size
    is 5 GB per S3 restriction).
  <li>
    You can't access S3 files when running Pig/Grunt in non-Hadoop local mode
    (<code>pig -x local</code>).
  <li>
    So you can download S3 files yourself, for local testing: 
    <code>s3n://blah</code> in hadoop = 
    <code>http://blah.s3.amazonaws.com</code> on the web.
    Show the XML directory listing at <a href="http://uw-cse344-test.s3.amazonaws.com/">
<code>http://uw-cse344-test.s3.amazonaws.com/</code></a> ,
    and download <a href="http://uw-cse344-test.s3.amazonaws.com/cse344-test-file">
<code>http://uw-cse344-test.s3.amazonaws.com/cse344-test-file</code></a> .
</ul>
<em>Output:</em> you have two options for where to put your output files.
<ul>
<li>
  Easier and expensive way: create your own S3 bucket (file system), 
  write the output data there.  Output filenames become <code>s3n://your-bucket/outdir</code> .  Can download the files via S3 Management Console.  But S3 does cost
  money, even when the data isn't going anywhere.  DELETE YOUR DATA ONCE YOU'RE
  DONE !!!
<li>
  Harder and cheapskate way: write to cluster's HDFS.  Output filenames
  are <code>/outdir</code>, or <code>/user/hadoop/outdir</code>.  (you'll need to create <code>/user/hadoop</code> .)
  Need to double download - from HDFS to master node's filesystem with
  <code>hadoop dfs -copyToLocal</code>, then from there to local machine with scp.
</ul>
<p>I will show using S3 first, and then HDFS if we have time.<br>
<ul>
<li>Open the S3 Management Console: <a href="https://console.aws.amazon.com/s3/home">https://console.aws.amazon.com/s3/home</a>
<li>Select the bucket, show the output folder from the prior run.
<li>Why a folder?  Each Hadoop task (a single node's worth of map or reduce operations) that produces final output writes a separate output file.  This is
to avoid contention on a single output file which would destroy the concurrency.
You can just concatenate all the files together after you download them.
<li>
  Select the folder, download <em>each</em> output file.
  Can get excruciating?
<li>
  Adventurous?  Get an S3 desktop or command-line client (I use Cyberduck).
  You'll need to <a href="https://aws-portal.amazon.com/gp/aws/developer/account/index.html?ie=UTF8&action=access-key">
    create an AWS secret access key</a>, which is separate from your
  EC2/Elastic MR SSH key pair (confused yet?), and login with that.
</ul>


<p>30 mins: Running Pig jobs on the cluster
<ul>
  <li>Show Grunt interactive mode: run <code>pig</code>, and cut
    and paste commands from <code>example.pig</code>. <br>
    Show <code>DUMP</code>, <code>DESCRIBE</code>, <code>HELP</code>
    along the way.  Don't <code>STORE</code> - except to note that
    this will fail because Pig refuses to overwrite old data (smart, but
    un-Unix-like).<br>
    <em>But don't go through each Pig command in detail - refer to lecture</em>
  <li>Upload <code>example.pig</code> to the master node:<br>
    <code>scp i .ssh/AmazonEC2-michaelr-20100730.pem hadoop@ec2-174-129-100-100.compute-1.amazonaws.com</code>
  <li>
    Delete the old results directory from the S3 bucket
    in S3 Management Console.
  <li>Begin running the script on the cluster:<br>
    <code>pig example.pig</code>
</ul>

<p>While waiting for the script to finish: Using the job tracker's web UI
<ul>
  <li>Open another ssh to the master node, then show the job tracker website
 using <code>lynx</code> 
  <ul>
    <li>See running, failed, completed jobs - percentage completed
    <li>Click through to see each job - graph statistics for map, reduce tasks
    <li>View nodes, see tasks running on each node
  </ul>
  <li>View the job tracker from local Firefox
  <ul>
    <li><a href="https://addons.mozilla.org/en-US/firefox/addon/2464/">
      Install FoxyProxy</a>
    <li><a href="http://support.mozilla.com/kb/profiles">Copy in</a>
      the <code>foxyproxy.xml</code> from the project archive
    <li>Create the ssh-tunneled proxy:
      <code>ssh -fND 8157 -i .ssh/AmazonEC2-michaelr-20100730.pem hadoop@ec2-174-129-100-100.compute-1.amazonaws.com</code>
  </ul>
  </li>

  <li>
    Canceling a Pig run - Ctrl-C to exit Pig, then you may want 
    to kill any runaway Hadoop jobs that are left over.  In another
    ssh connection to the master node, use the job tracker to find the
    runaway jobs, then kill them with <code>hadoop job -kill JOB_NAME</code>.
  </li>
</ul>

<p>5 mins: Running Pig locally
<ul>
  <li>Set up Unix environment for the project - download
    cse344-test-file, myudfs.jar (don't compile myudfs.jar yourself)
  <li>Run the sample script <code>example-local.pig</code>.
    Note the changes from example.pig.  Also note:
    <code><b>java -jar pigtest/lib/pig.jar -x local</b> example-local.pig</code>
  <li>Note that the output gets sent to one file, not multiple.
</ul>

</body>
</html>
